#!/bin/bash
#PBS -N Spark
#PBS -l walltime=00:59:00
#PBS -l select=2:ncpus=36
#PBS -l place=scatter:excl
#PBS -A sc048
 
module load spack/cirrus
module load jdk-8u92-linux-x64-gcc-6.2.0-24xtmiy
module load anaconda/python3
source activate cirrus-py36

export SPARK_HOME=$HOME/spark-2.4.0-bin-hadoop2.7
export SPARK_MASTER_HOST=$HOSTNAME
export SPARK_MASTER_PORT=7077
export SPARK_MASTER_WEBUI_PORT=8080
export PATH=$SPARK_HOME/sbin:$SPARK_HOME/bin:$PATH

export NUM_NODES=2

cd $HOME/bash_scripts
rm -f master.log
rm -f driver.log
rm -f worker.log
rm -f slaves.log

nodes=($( cat $PBS_NODEFILE | sort | uniq ))
nnodes=${#nodes[@]}
last=$(( $nnodes - 1 ))

echo "`hostname`" > master.log
for each in "${nodes[@]}"
do
  echo "Nodo: $each"
done
 
# start resource manager only once
./start_master.sh
mastername=$(cat "master.log") 
echo "Started master on" $mastername

drivername="NONE"

# start workers in all the nodes except the one where the master and driver were started
for i in "${nodes[@]}"

do
    echo $i
    ssh $i "cd $HOME/bash_scripts; ./start_worker.sh $mastername $drivername" &
done


sleep 1h


